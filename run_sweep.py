#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å¤šç»„è¶…å‚æ•°å®éªŒè¿è¡Œå™¨
====================
æ”¯æŒç½‘æ ¼æœç´¢å’Œæ‰‹åŠ¨æŒ‡å®šå®éªŒç»„ï¼Œè‡ªåŠ¨è¿è¡Œå¹¶æ±‡æ€»ç»“æœã€‚

éµå¾ªå®éªŒæ—¥å¿—è§„èŒƒï¼š
- æ¯ç»„å®éªŒç‹¬ç«‹ç›®å½•
- è‡ªåŠ¨è®°å½•æ‰€æœ‰é…ç½®
- ç”Ÿæˆæ±‡æ€»å¯¹æ¯”æŠ¥å‘Š
"""

import os
import sys
import yaml
import json
import time
import itertools
import subprocess
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any
import copy


def load_sweep_config(config_path: str) -> Dict:
    """åŠ è½½è¶…å‚æ•°æœç´¢é…ç½®"""
    with open(config_path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


def generate_grid_experiments(sweep_config: Dict) -> List[Dict]:
    """
    ç½‘æ ¼æœç´¢ï¼šç”Ÿæˆæ‰€æœ‰å‚æ•°ç»„åˆ
    """
    grid_params = sweep_config["grid_search"]
    fixed_params = sweep_config.get("fixed_params", {})
    
    # è·å–æ‰€æœ‰å‚æ•°åå’Œå€¼åˆ—è¡¨
    param_names = list(grid_params.keys())
    param_values = [grid_params[name] for name in param_names]
    
    # ç”Ÿæˆæ‰€æœ‰ç»„åˆ
    experiments = []
    for combo in itertools.product(*param_values):
        exp_params = dict(zip(param_names, combo))
        
        # åº”ç”¨å›ºå®šå‚æ•°
        if "lora_dropout" not in exp_params and "lora_dropout" in fixed_params:
            exp_params["lora_dropout"] = fixed_params["lora_dropout"]
        if "batch_size" not in exp_params and "batch_size" in fixed_params:
            exp_params["batch_size"] = fixed_params["batch_size"]
        if "seed" not in exp_params and "seed" in fixed_params:
            exp_params["seed"] = fixed_params["seed"]
        if "weight_decay" not in exp_params and "weight_decay" in fixed_params:
            exp_params["weight_decay"] = fixed_params["weight_decay"]
        
        # è‡ªåŠ¨è®¡ç®— lora_alpha (å¦‚æœé…ç½®äº† ratio)
        if "lora_alpha" not in exp_params and "lora_alpha_ratio" in fixed_params:
            exp_params["lora_alpha"] = exp_params.get("lora_rank", 16) * fixed_params["lora_alpha_ratio"]
        
        # ç”Ÿæˆå®éªŒåç§°
        name_parts = []
        if "learning_rate" in exp_params:
            name_parts.append(f"lr{exp_params['learning_rate']}")
        if "lora_rank" in exp_params:
            name_parts.append(f"r{exp_params['lora_rank']}")
        if "lora_alpha" in exp_params:
            name_parts.append(f"a{exp_params['lora_alpha']}")
        if "batch_size" in exp_params:
            name_parts.append(f"bs{exp_params['batch_size']}")
        if "n_epochs" in exp_params:
            name_parts.append(f"ep{exp_params['n_epochs']}")
        
        exp_params["name"] = "_".join(name_parts)
        experiments.append(exp_params)
    
    return experiments


def generate_manual_experiments(sweep_config: Dict) -> List[Dict]:
    """
    æ‰‹åŠ¨æ¨¡å¼ï¼šç›´æ¥ä½¿ç”¨æŒ‡å®šçš„å®éªŒç»„
    """
    return sweep_config["manual_experiments"]


def merge_config(base_config: Dict, exp_params: Dict) -> Dict:
    """
    åˆå¹¶åŸºç¡€é…ç½®å’Œå®éªŒå‚æ•°
    """
    config = copy.deepcopy(base_config)
    
    # æ˜ å°„å®éªŒå‚æ•°åˆ°é…ç½®ç»“æ„
    param_mapping = {
        "learning_rate": ("training", "learning_rate"),
        "batch_size": ("training", "batch_size"),
        "n_epochs": ("training", "n_epochs"),
        "weight_decay": ("training", "weight_decay"),
        "lora_rank": ("lora", "rank"),
        "lora_alpha": ("lora", "alpha"),
        "lora_dropout": ("lora", "dropout"),
        "seed": (None, "seed"),
    }
    
    for param_name, value in exp_params.items():
        if param_name == "name":
            continue
        if param_name in param_mapping:
            section, key = param_mapping[param_name]
            if section is None:
                config[key] = value
            else:
                if section not in config:
                    config[section] = {}
                config[section][key] = value
    
    # è®¾ç½®å®éªŒåç§°
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    config["output"]["experiment_name"] = f"{timestamp}_{exp_params['name']}"
    
    # SwanLab å®éªŒåç§°
    if config.get("swanlab", {}).get("enabled"):
        config["swanlab"]["experiment_name"] = exp_params["name"]
    
    return config


def run_single_experiment(config: Dict, exp_index: int, total: int, script_path: str) -> Dict:
    """
    è¿è¡Œå•ä¸ªå®éªŒ
    """
    exp_name = config["output"]["experiment_name"]
    print(f"\n{'='*60}")
    print(f"ğŸš€ Running Experiment [{exp_index + 1}/{total}]: {exp_name}")
    print(f"{'='*60}")
    
    # ä¿å­˜ä¸´æ—¶é…ç½®æ–‡ä»¶
    temp_config_path = Path(script_path).parent / f".temp_config_{exp_index}.yaml"
    with open(temp_config_path, "w", encoding="utf-8") as f:
        yaml.dump(config, f, allow_unicode=True, default_flow_style=False)
    
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    
    # è¿è¡Œè®­ç»ƒè„šæœ¬
    result = {
        "name": exp_name,
        "config": config,
        "status": "unknown",
        "duration": 0,
        "error": None,
        "metrics": {},
    }
    
    try:
        process = subprocess.run(
            [sys.executable, script_path, "--config", str(temp_config_path)],
            capture_output=False,  # ç›´æ¥è¾“å‡ºåˆ°ç»ˆç«¯
            text=True,
        )
        
        if process.returncode == 0:
            result["status"] = "success"
            print(f"âœ… Experiment {exp_name} completed successfully!")
        else:
            result["status"] = "failed"
            result["error"] = f"Return code: {process.returncode}"
            print(f"âŒ Experiment {exp_name} failed!")
            
    except Exception as e:
        result["status"] = "error"
        result["error"] = str(e)
        print(f"âŒ Experiment {exp_name} error: {e}")
    
    # è®°å½•è€—æ—¶
    result["duration"] = time.time() - start_time
    
    # æ¸…ç†ä¸´æ—¶é…ç½®
    if temp_config_path.exists():
        temp_config_path.unlink()
    
    # å°è¯•è¯»å–è®­ç»ƒç»“æœ
    exp_dir = Path(config["output"]["work_dir"]) / exp_name
    trainer_state_path = exp_dir / "checkpoints" / "trainer_state.json"
    if trainer_state_path.exists():
        try:
            with open(trainer_state_path, "r") as f:
                trainer_state = json.load(f)
                if trainer_state.get("best_metric"):
                    result["metrics"]["best_metric"] = trainer_state["best_metric"]
                if trainer_state.get("log_history"):
                    # è·å–æœ€åçš„è®­ç»ƒ loss
                    for log in reversed(trainer_state["log_history"]):
                        if "loss" in log:
                            result["metrics"]["final_loss"] = log["loss"]
                            break
        except Exception:
            pass
    
    return result


def generate_summary_report(results: List[Dict], output_dir: Path):
    """
    ç”Ÿæˆå®éªŒæ±‡æ€»æŠ¥å‘Š
    """
    report_path = output_dir / "sweep_summary.md"
    
    lines = []
    lines.append("# è¶…å‚æ•°æœç´¢å®éªŒæ±‡æ€»æŠ¥å‘Š")
    lines.append(f"\nç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    # ç»Ÿè®¡
    total = len(results)
    success = sum(1 for r in results if r["status"] == "success")
    failed = total - success
    
    lines.append(f"## ğŸ“Š æ€»ä½“ç»Ÿè®¡\n")
    lines.append(f"- æ€»å®éªŒæ•°: {total}")
    lines.append(f"- æˆåŠŸ: {success}")
    lines.append(f"- å¤±è´¥: {failed}")
    lines.append(f"- æ€»è€—æ—¶: {sum(r['duration'] for r in results) / 3600:.2f} å°æ—¶\n")
    
    # ç»“æœè¡¨æ ¼
    lines.append("## ğŸ“‹ å®éªŒç»“æœ\n")
    lines.append("| å®éªŒåç§° | çŠ¶æ€ | è€—æ—¶ | Final Loss | LR | Rank | Dropout | WD | Epochs |")
    lines.append("|----------|------|------|------------|-----|------|---------|-----|--------|")
    
    for r in results:
        status_icon = "âœ…" if r["status"] == "success" else "âŒ"
        duration = f"{r['duration']/60:.1f}min"
        final_loss = r["metrics"].get("final_loss", "N/A")
        if isinstance(final_loss, float):
            final_loss = f"{final_loss:.4f}"
        
        # æå–å‚æ•°
        lr = r["config"]["training"].get("learning_rate", "N/A")
        rank = r["config"]["lora"].get("rank", "N/A")
        dropout = r["config"]["lora"].get("dropout", "N/A")
        wd = r["config"]["training"].get("weight_decay", "N/A")
        epochs = r["config"]["training"].get("n_epochs", "N/A")
        
        lines.append(f"| {r['name'][:25]} | {status_icon} | {duration} | {final_loss} | {lr} | {rank} | {dropout} | {wd} | {epochs} |")
    
    # æœ€ä½³å®éªŒ
    successful_results = [r for r in results if r["status"] == "success" and r["metrics"].get("final_loss")]
    if successful_results:
        best = min(successful_results, key=lambda x: x["metrics"]["final_loss"])
        lines.append(f"\n## ğŸ† æœ€ä½³å®éªŒ\n")
        lines.append(f"**{best['name']}**")
        lines.append(f"- Final Loss: {best['metrics']['final_loss']:.4f}")
        lines.append(f"- é…ç½®:")
        lines.append(f"  - Learning Rate: {best['config']['training']['learning_rate']}")
        lines.append(f"  - LoRA Rank: {best['config']['lora']['rank']}")
        lines.append(f"  - LoRA Alpha: {best['config']['lora']['alpha']}")
        lines.append(f"  - LoRA Dropout: {best['config']['lora'].get('dropout', 'N/A')}")
        lines.append(f"  - Weight Decay: {best['config']['training'].get('weight_decay', 'N/A')}")
        lines.append(f"  - Batch Size: {best['config']['training']['batch_size']}")
        lines.append(f"  - Epochs: {best['config']['training']['n_epochs']}")
    
    # å¤±è´¥å®éªŒ
    failed_results = [r for r in results if r["status"] != "success"]
    if failed_results:
        lines.append(f"\n## âš ï¸ å¤±è´¥å®éªŒ\n")
        for r in failed_results:
            lines.append(f"- **{r['name']}**: {r.get('error', 'Unknown error')}")
    
    # å†™å…¥æŠ¥å‘Š
    with open(report_path, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))
    
    print(f"\nğŸ“ Summary report saved to: {report_path}")
    
    # åŒæ—¶ä¿å­˜ JSON æ ¼å¼ç»“æœ
    json_path = output_dir / "sweep_results.json"
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False, default=str)
    
    return report_path


def check_completed_experiments(work_dir: Path, experiments: List[Dict]) -> List[Dict]:
    """
    æ£€æŸ¥å·²å®Œæˆçš„å®éªŒï¼Œè¿”å›æœªå®Œæˆçš„å®éªŒåˆ—è¡¨
    """
    pending = []
    for exp in experiments:
        # æ£€æŸ¥æ˜¯å¦æœ‰åŒ¹é…çš„å®éªŒç›®å½•
        exp_pattern = f"*_{exp['name']}"
        matching_dirs = list(work_dir.glob(exp_pattern))
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ best_model ç›®å½•ï¼ˆè¡¨ç¤ºè®­ç»ƒå®Œæˆï¼‰
        completed = False
        for d in matching_dirs:
            if (d / "best_model").exists():
                completed = True
                print(f"â­ï¸  Skipping completed experiment: {exp['name']}")
                break
        
        if not completed:
            pending.append(exp)
    
    return pending


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="å¤šç»„è¶…å‚æ•°å®éªŒè¿è¡Œå™¨")
    parser.add_argument(
        "--config", "-c",
        type=str,
        default="sweep_config.yaml",
        help="è¶…å‚æ•°æœç´¢é…ç½®æ–‡ä»¶"
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="åªæ‰“å°å®éªŒè®¡åˆ’ï¼Œä¸å®é™…è¿è¡Œ"
    )
    parser.add_argument(
        "--start-from",
        type=int,
        default=0,
        help="ä»ç¬¬å‡ ä¸ªå®éªŒå¼€å§‹ï¼ˆç”¨äºæ–­ç‚¹ç»­è·‘ï¼‰"
    )
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    sweep_config = load_sweep_config(args.config)
    base_config = sweep_config["base_config"]
    run_control = sweep_config["run_control"]
    
    # ç”Ÿæˆå®éªŒåˆ—è¡¨
    if sweep_config["search_mode"] == "grid_search":
        experiments = generate_grid_experiments(sweep_config)
        print(f"ğŸ“Š Grid Search Mode: {len(experiments)} experiments generated")
    else:
        experiments = generate_manual_experiments(sweep_config)
        print(f"ğŸ“‹ Manual Mode: {len(experiments)} experiments specified")
    
    # é™åˆ¶å®éªŒæ•°é‡
    max_exp = run_control.get("max_experiments", 100)
    if len(experiments) > max_exp:
        print(f"âš ï¸  Limiting to first {max_exp} experiments (from {len(experiments)})")
        experiments = experiments[:max_exp]
    
    # è·³è¿‡å·²å®Œæˆçš„å®éªŒ
    work_dir = Path(base_config["output"]["work_dir"])
    work_dir.mkdir(parents=True, exist_ok=True)
    
    if run_control.get("skip_completed", True):
        experiments = check_completed_experiments(work_dir, experiments)
        print(f"ğŸ“ {len(experiments)} experiments remaining")
    
    # ä»æŒ‡å®šä½ç½®å¼€å§‹
    if args.start_from > 0:
        experiments = experiments[args.start_from:]
        print(f"â© Starting from experiment {args.start_from}")
    
    if not experiments:
        print("âœ… All experiments completed!")
        return
    
    # æ‰“å°å®éªŒè®¡åˆ’
    print(f"\n{'='*60}")
    print("ğŸ“‹ Experiment Plan")
    print(f"{'='*60}")
    for i, exp in enumerate(experiments):
        print(f"  [{i+1}] {exp['name']}")
        print(f"      lr={exp.get('learning_rate', 'default')}, "
              f"rank={exp.get('lora_rank', 'default')}, "
              f"bs={exp.get('batch_size', 'default')}, "
              f"epochs={exp.get('n_epochs', 'default')}")
    
    if args.dry_run:
        print(f"\nğŸ” Dry run complete. {len(experiments)} experiments would be run.")
        return
    
    # ç¡®è®¤è¿è¡Œ
    print(f"\n{'='*60}")
    print(f"ğŸš€ Ready to run {len(experiments)} experiments")
    print(f"{'='*60}")
    
    response = input("Continue? [y/N]: ").strip().lower()
    if response != "y":
        print("Aborted.")
        return
    
    # è¿è¡Œå®éªŒ
    script_path = Path(__file__).parent / "train_qwen8b_lora.py"
    results = []
    
    for i, exp_params in enumerate(experiments):
        # åˆå¹¶é…ç½®
        config = merge_config(base_config, exp_params)
        
        # è¿è¡Œå®éªŒ
        result = run_single_experiment(config, i, len(experiments), str(script_path))
        results.append(result)
        
        # æ£€æŸ¥æ˜¯å¦ç»§ç»­
        if result["status"] != "success" and not run_control.get("continue_on_failure", True):
            print("âŒ Stopping due to failure (continue_on_failure=false)")
            break
        
        # å®éªŒé—´éš”
        if i < len(experiments) - 1:
            interval = run_control.get("interval_seconds", 30)
            print(f"â³ Waiting {interval}s before next experiment...")
            time.sleep(interval)
    
    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    generate_summary_report(results, work_dir)
    
    print(f"\n{'='*60}")
    print("ğŸ‰ All experiments completed!")
    print(f"{'='*60}")


if __name__ == "__main__":
    main()
