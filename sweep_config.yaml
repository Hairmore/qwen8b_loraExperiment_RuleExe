# ============================================
# 多组超参数实验配置
# ============================================
# 数据规模: 1600-3600 条 (中等规模，需注意过拟合)
#
# 推荐策略：两阶段搜索
# 阶段1: 搜索 lr + rank + epochs (核心参数)
# 阶段2: 基于最佳配置，搜索正则化参数

# 搜索模式: grid_search / manual
search_mode: "manual"  # 推荐手动模式，更高效

# ============================================
# 基础配置 (所有实验共享)
# ============================================
base_config:
  model:
    name: "/media/hairmore/LaCie/qwen3-8b/Qwen3-8B"
    trust_remote_code: true

  data:
    train_file: "data/train_enesvi_nat40.jsonl"
    eval_file: null                   # null 表示从 train 切分
    max_length: 8192
    split:
      enabled: true
      eval_ratio: 0.1
      seed: 42

  lora:
    target_modules:
      - "q_proj"
      - "k_proj"
      - "v_proj"
      - "o_proj"
      - "gate_proj"
      - "up_proj"
      - "down_proj"

  training:
    gradient_accumulation_steps: 2
    lr_scheduler_type: "cosine"
    warmup_ratio: 0.05
    eval_steps: 50
    save_steps: 100
    logging_steps: 10
    fp16: true
    gradient_checkpointing: true

  output:
    work_dir: "/media/hairmore/LaCie/qwen8b_loraExperiment_RuleExe/work_dirs"

  swanlab:
    enabled: true
    project: "qwen8b-lora-sweep_enesvi"

# ============================================
# 手动指定实验组 (推荐)
# ============================================
# 阶段1: 核心参数搜索 (9组)
# 目标: 找到最佳 learning_rate 和 lora_rank 组合

manual_experiments:
  # -------- 阶段1: 核心参数 --------
  # Group A: 高学习率 (1e-4)
  - name: "lr1e4_r16"
    learning_rate: 1.0e-4
    lora_rank: 16
    lora_alpha: 32
    lora_dropout: 0.1
    batch_size: 8
    n_epochs: 2
    weight_decay: 0.01
    seed: 42

  - name: "lr1e4_r32"
    learning_rate: 1.0e-4
    lora_rank: 32
    lora_alpha: 64
    lora_dropout: 0.1
    batch_size: 8
    n_epochs: 2
    weight_decay: 0.01
    seed: 42

  - name: "lr1e4_r8"
    learning_rate: 1.0e-4
    lora_rank: 8
    lora_alpha: 16
    lora_dropout: 0.1
    batch_size: 8
    n_epochs: 2
    weight_decay: 0.01
    seed: 42

  # Group B: 中学习率 (5e-5) - 通常最稳定
  - name: "lr5e5_r16"
    learning_rate: 5.0e-5
    lora_rank: 16
    lora_alpha: 32
    lora_dropout: 0.1
    batch_size: 8
    n_epochs: 3
    weight_decay: 0.01
    seed: 42

  - name: "lr5e5_r32"
    learning_rate: 5.0e-5
    lora_rank: 32
    lora_alpha: 64
    lora_dropout: 0.1
    batch_size: 8
    n_epochs: 3
    weight_decay: 0.01
    seed: 42

  - name: "lr5e5_r8"
    learning_rate: 5.0e-5
    lora_rank: 8
    lora_alpha: 16
    lora_dropout: 0.1
    batch_size: 8
    n_epochs: 3
    weight_decay: 0.01
    seed: 42

  # Group C: 低学习率 (2e-5) - 更保守
  - name: "lr2e5_r16"
    learning_rate: 2.0e-5
    lora_rank: 16
    lora_alpha: 32
    lora_dropout: 0.1
    batch_size: 8
    n_epochs: 3
    weight_decay: 0.01
    seed: 42

  - name: "lr2e5_r32"
    learning_rate: 2.0e-5
    lora_rank: 32
    lora_alpha: 64
    lora_dropout: 0.1
    batch_size: 8
    n_epochs: 3
    weight_decay: 0.01
    seed: 42

  - name: "lr2e5_r64"
    learning_rate: 2.0e-5
    lora_rank: 64
    lora_alpha: 128
    lora_dropout: 0.1
    batch_size: 4          # 降低 bs 以适应更大 rank
    n_epochs: 3
    weight_decay: 0.01
    seed: 42

  # -------- 阶段2: 正则化参数 --------
  # 基于阶段1最佳配置，测试不同正则化强度
  # （先跑完阶段1，根据结果修改下面的 base 配置）
  
  # 强正则化 (防过拟合)
  - name: "best_strong_reg"
    learning_rate: 5.0e-5   # ← 改成阶段1最佳
    lora_rank: 16           # ← 改成阶段1最佳
    lora_alpha: 32
    lora_dropout: 0.2       # 较高 dropout
    batch_size: 8
    n_epochs: 3
    weight_decay: 0.05      # 较高 weight_decay
    seed: 42

  # 中等正则化
  - name: "best_medium_reg"
    learning_rate: 5.0e-5
    lora_rank: 16
    lora_alpha: 32
    lora_dropout: 0.15
    batch_size: 8
    n_epochs: 3
    weight_decay: 0.02
    seed: 42

  # 弱正则化 (如果数据质量高、多样性好)
  - name: "best_light_reg"
    learning_rate: 5.0e-5
    lora_rank: 16
    lora_alpha: 32
    lora_dropout: 0.05
    batch_size: 8
    n_epochs: 2             # 减少 epoch 也能防过拟合
    weight_decay: 0.01
    seed: 42

# ============================================
# 网格搜索参数空间 (备用)
# ============================================
# 如果想用网格搜索，改 search_mode: "grid_search"
# 当前组合: 3 × 3 × 2 = 18 组

grid_search:
  learning_rate: [1.0e-4, 5.0e-5, 2.0e-5]
  lora_rank: [8, 16, 32]
  n_epochs: [2, 3]

fixed_params:
  lora_alpha_ratio: 2
  lora_dropout: 0.1
  batch_size: 8
  weight_decay: 0.01
  seed: 42

# ============================================
# 运行控制
# ============================================
run_control:
  max_experiments: 20
  continue_on_failure: true
  interval_seconds: 30
  skip_completed: true
